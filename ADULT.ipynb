{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pandas import read_csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['age',\n",
    "            'wrk-cls',\n",
    "            'fnlwgt',\n",
    "            'edu-lvl',\n",
    "            'edu-num',\n",
    "            'marriage',\n",
    "            'occupation',\n",
    "            'relationship',\n",
    "            'race',\n",
    "            'sex',\n",
    "            'cap-gain',\n",
    "            'cap-loss',\n",
    "            'hr-per-wk',\n",
    "            'country',\n",
    "            'income']\n",
    "\n",
    "df = pd.read_csv('adult.data', header = None, names = headers, na_values = ' ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the labels with 0's and 1's\n",
    "numeric = df['income'].tolist()\n",
    "for i, j in enumerate(numeric):\n",
    "    if j == ' >50K':\n",
    "        numeric[i] = 1\n",
    "    else:\n",
    "        numeric[i] = 0\n",
    "        \n",
    "df['income'] = numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating features and labels\n",
    "X_p = pd.get_dummies(df.iloc[:, :-1])\n",
    "y_p = df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy vectors for train set\n",
    "logreg_accuracy_train = []\n",
    "randforest_accuracy_train = []\n",
    "knn_accuracy_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy vectors for test set\n",
    "logreg_accuracy = []\n",
    "randforest_accuracy = []\n",
    "knn_accuracy = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# take all our penguin data, and reserve 20% of it for testing \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_p, y_p,\n",
    "                                                    train_size=5000,\n",
    "                                                    random_state=12345,\n",
    "                                                    stratify=y_p)\n",
    "\n",
    "\n",
    "# Initializing Classifiers\n",
    "clf1 = KNeighborsClassifier()\n",
    "clf2 = RandomForestClassifier(n_estimators = 1024)\n",
    "clf3 = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Declaring parameters\n",
    "K_list = np.array([n*20 for n in range(1,26)])\n",
    "F_list = np.array([1, 2, 4, 6, 8, 12, 16, 20])\n",
    "C_list = np.array([10**(-8), 10**(-7), 10**(-6), 10**(-5), 10**(-4), 10**(-3), 10**(-2), 10**(-1), \n",
    "                       10**(0), 10**(1), 10**(2), 10**(3), 10**(4)])\n",
    "\n",
    "# Building the pipelines\n",
    "pipe1 = Pipeline([('std', StandardScaler()),\n",
    "                  ('classifier', clf1)])\n",
    "\n",
    "pipe2 = Pipeline([('std', StandardScaler()),\n",
    "                  ('classifier', clf2)])\n",
    "\n",
    "pipe3 = Pipeline([('std', StandardScaler()),\n",
    "                  ('classifier', clf3)])\n",
    "\n",
    "\n",
    "# Setting up the parameter grids\n",
    "param_grid1 = [{'classifier__weights': ['uniform', 'distance'],\n",
    "                'classifier__n_neighbors': K_list}]\n",
    "\n",
    "param_grid2 = [{'classifier__max_features': F_list}]\n",
    "\n",
    "param_grid3 = [{'classifier__C': C_list}]\n",
    "\n",
    "\n",
    "# Setting up multiple GridSearchCV objects, 1 for each algorithm\n",
    "gridcvs = {}\n",
    "\n",
    "for pgrid, est, name in zip((param_grid1, param_grid2, param_grid3),\n",
    "                            (pipe1, pipe2, pipe3),\n",
    "                            ('KNN', 'RandomForest', 'Logistic')):\n",
    "    gcv = GridSearchCV(estimator=est,\n",
    "                       param_grid=pgrid,\n",
    "                       scoring='accuracy',\n",
    "                       n_jobs=3,\n",
    "                       cv=5, # just 2-fold inner loop, i.e. train/test\n",
    "                       verbose=0,\n",
    "                       return_train_score=True,\n",
    "                       refit='accuracy')\n",
    "    gridcvs[name] = gcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer fold 1/5 | tuning KNN      | inner ACC 82.80% | outer ACC 82.90%\n",
      "outer fold 1/5 | tuning Logistic | inner ACC 84.08% | outer ACC 84.80%\n",
      "outer fold 1/5 | tuning RandomForest | inner ACC 85.17% | outer ACC 86.20%\n",
      "outer fold 2/5 | tuning KNN      | inner ACC 82.83% | outer ACC 82.40%\n",
      "outer fold 2/5 | tuning Logistic | inner ACC 84.47% | outer ACC 84.00%\n",
      "outer fold 2/5 | tuning RandomForest | inner ACC 85.52% | outer ACC 84.60%\n",
      "outer fold 3/5 | tuning KNN      | inner ACC 82.80% | outer ACC 82.50%\n",
      "outer fold 3/5 | tuning Logistic | inner ACC 84.35% | outer ACC 84.10%\n",
      "outer fold 3/5 | tuning RandomForest | inner ACC 85.60% | outer ACC 84.30%\n",
      "outer fold 4/5 | tuning KNN      | inner ACC 83.05% | outer ACC 82.10%\n",
      "outer fold 4/5 | tuning Logistic | inner ACC 84.10% | outer ACC 83.60%\n",
      "outer fold 4/5 | tuning RandomForest | inner ACC 85.38% | outer ACC 85.50%\n",
      "outer fold 5/5 | tuning KNN      | inner ACC 82.88% | outer ACC 83.40%\n",
      "outer fold 5/5 | tuning Logistic | inner ACC 84.25% | outer ACC 84.30%\n",
      "outer fold 5/5 | tuning RandomForest | inner ACC 85.28% | outer ACC 84.80%\n",
      "CPU times: user 51.8 s, sys: 4.93 s, total: 56.8 s\n",
      "Wall time: 1h 3min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# ^^ this handy Jupyter magic times the execution of the cell for you\n",
    "\n",
    "cv_scores = {name: [] for name, gs_est in gridcvs.items()}\n",
    "\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# The outer loop for algorithm selection\n",
    "c = 1\n",
    "for outer_train_idx, outer_valid_idx in skfold.split(X_train,y_train):\n",
    "    for name, gs_est in sorted(gridcvs.items()):\n",
    "        print('outer fold %d/5 | tuning %-8s' % (c, name), end='')\n",
    "\n",
    "        # The inner loop for hyperparameter tuning\n",
    "        gs_est.fit(X_train.iloc[outer_train_idx], y_train.iloc[outer_train_idx])\n",
    "        y_pred = gs_est.predict(X_train.iloc[outer_valid_idx])\n",
    "        acc = accuracy_score(y_true=y_train.iloc[outer_valid_idx], y_pred=y_pred)\n",
    "        print(' | inner ACC %.2f%% | outer ACC %.2f%%' %\n",
    "              (gs_est.best_score_ * 100, acc * 100))\n",
    "        cv_scores[name].append(acc)\n",
    "\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN      | outer CV acc. 82.66% +\\- 0.450\n",
      "RandomForest | outer CV acc. 85.08% +\\- 0.685\n",
      "Logistic | outer CV acc. 84.16% +\\- 0.393\n",
      "\n",
      "KNN best parameters {'classifier__n_neighbors': 80, 'classifier__weights': 'distance'}\n",
      "RandomForest best parameters {'classifier__max_features': 12}\n",
      "Logistic best parameters {'classifier__C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Looking at the results\n",
    "for name in cv_scores:\n",
    "    print('%-8s | outer CV acc. %.2f%% +\\- %.3f' % (\n",
    "          name, 100 * np.mean(cv_scores[name]), 100 * np.std(cv_scores[name])))\n",
    "print()\n",
    "for name in cv_scores:\n",
    "    print('{} best parameters'.format(name), gridcvs[name].best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 83.10% (average over CV test folds)\n",
      "Best Parameters: {'classifier__n_neighbors': 80, 'classifier__weights': 'distance'}\n",
      "Training Accuracy: 100.00%\n",
      "Test Accuracy: 82.37%\n",
      "\n",
      "Accuracy 85.76% (average over CV test folds)\n",
      "Best Parameters: {'classifier__max_features': 16}\n",
      "Training Accuracy: 100.00%\n",
      "Test Accuracy: 85.01%\n",
      "\n",
      "Accuracy 84.36% (average over CV test folds)\n",
      "Best Parameters: {'classifier__C': 1.0}\n",
      "Training Accuracy: 85.28%\n",
      "Test Accuracy: 84.25%\n"
     ]
    }
   ],
   "source": [
    "# Fitting a model to the whole training set\n",
    "# using the \"best\" KNN algorithm\n",
    "best_algo = gridcvs['KNN']\n",
    "\n",
    "best_algo.fit(X_train, y_train)\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "print('Accuracy %.2f%% (average over CV test folds)' %\n",
    "      (100 * best_algo.best_score_))\n",
    "print('Best Parameters: %s' % gridcvs['KNN'].best_params_)\n",
    "print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "print('Test Accuracy: %.2f%%' % (100 * test_acc))\n",
    "knn_accuracy.append(test_acc)\n",
    "knn_accuracy_train.append(train_acc)\n",
    "\n",
    "# using the \"best\" RandomForest algorithm\n",
    "best_algo = gridcvs['RandomForest']\n",
    "\n",
    "best_algo.fit(X_train, y_train)\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "print('\\nAccuracy %.2f%% (average over CV test folds)' %\n",
    "      (100 * best_algo.best_score_))\n",
    "print('Best Parameters: %s' % gridcvs['RandomForest'].best_params_)\n",
    "print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "print('Test Accuracy: %.2f%%' % (100 * test_acc))\n",
    "randforest_accuracy.append(test_acc)\n",
    "randforest_accuracy_train.append(train_acc)\n",
    "\n",
    "# using the \"best\" Logistic algorithm\n",
    "best_algo = gridcvs['Logistic']\n",
    "\n",
    "best_algo.fit(X_train, y_train)\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "\n",
    "print('\\nAccuracy %.2f%% (average over CV test folds)' %\n",
    "      (100 * best_algo.best_score_))\n",
    "print('Best Parameters: %s' % gridcvs['Logistic'].best_params_)\n",
    "print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "print('Test Accuracy: %.2f%%' % (100 * test_acc))\n",
    "logreg_accuracy.append(test_acc)\n",
    "logreg_accuracy_train.append(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8237024083936094\n",
      "0.8501311501470471\n",
      "0.8424608536682299\n"
     ]
    }
   ],
   "source": [
    "print(knn_accuracy[0])\n",
    "print(randforest_accuracy[0])\n",
    "print(logreg_accuracy[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# take all our penguin data, and reserve 20% of it for testing \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_p, y_p,\n",
    "                                                    train_size=5000,\n",
    "                                                    random_state=54321,\n",
    "                                                    stratify=y_p)\n",
    "\n",
    "\n",
    "# Initializing Classifiers\n",
    "clf1 = KNeighborsClassifier()\n",
    "clf2 = RandomForestClassifier(n_estimators = 1024)\n",
    "clf3 = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Declaring parameters\n",
    "K_list = np.array([n*20 for n in range(1,26)])\n",
    "F_list = np.array([1, 2, 4, 6, 8, 12, 16, 20])\n",
    "C_list = np.array([10**(-8), 10**(-7), 10**(-6), 10**(-5), 10**(-4), 10**(-3), 10**(-2), 10**(-1), \n",
    "                       10**(0), 10**(1), 10**(2), 10**(3), 10**(4)])\n",
    "\n",
    "# Building the pipelines\n",
    "pipe1 = Pipeline([('std', StandardScaler()),\n",
    "                  ('classifier', clf1)])\n",
    "\n",
    "pipe2 = Pipeline([('std', StandardScaler()),\n",
    "                  ('classifier', clf2)])\n",
    "\n",
    "pipe3 = Pipeline([('std', StandardScaler()),\n",
    "                  ('classifier', clf3)])\n",
    "\n",
    "\n",
    "# Setting up the parameter grids\n",
    "param_grid1 = [{'classifier__weights': ['uniform', 'distance'],\n",
    "                'classifier__n_neighbors': K_list}]\n",
    "\n",
    "param_grid2 = [{'classifier__max_features': F_list}]\n",
    "\n",
    "param_grid3 = [{'classifier__C': C_list}]\n",
    "\n",
    "\n",
    "# Setting up multiple GridSearchCV objects, 1 for each algorithm\n",
    "gridcvs = {}\n",
    "\n",
    "for pgrid, est, name in zip((param_grid1, param_grid2, param_grid3),\n",
    "                            (pipe1, pipe2, pipe3),\n",
    "                            ('KNN', 'RandomForest', 'Logistic')):\n",
    "    gcv = GridSearchCV(estimator=est,\n",
    "                       param_grid=pgrid,\n",
    "                       scoring='accuracy',\n",
    "                       n_jobs=3,\n",
    "                       cv=5, # just 2-fold inner loop, i.e. train/test\n",
    "                       verbose=0,\n",
    "                       return_train_score=True,\n",
    "                       refit='accuracy')\n",
    "    gridcvs[name] = gcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer fold 1/5 | tuning KNN      | inner ACC 82.88% | outer ACC 83.10%\n",
      "outer fold 1/5 | tuning Logistic | inner ACC 84.78% | outer ACC 84.40%\n",
      "outer fold 1/5 | tuning RandomForest | inner ACC 85.35% | outer ACC 84.90%\n",
      "outer fold 2/5 | tuning KNN      | inner ACC 83.28% | outer ACC 82.10%\n",
      "outer fold 2/5 | tuning Logistic | inner ACC 84.40% | outer ACC 85.10%\n",
      "outer fold 2/5 | tuning RandomForest | inner ACC 85.62% | outer ACC 84.80%\n",
      "outer fold 3/5 | tuning KNN      | inner ACC 83.23% | outer ACC 82.70%\n",
      "outer fold 3/5 | tuning Logistic | inner ACC 84.40% | outer ACC 85.20%\n",
      "outer fold 3/5 | tuning RandomForest | inner ACC 84.95% | outer ACC 85.90%\n",
      "outer fold 4/5 | tuning KNN      | inner ACC 83.35% | outer ACC 82.60%\n",
      "outer fold 4/5 | tuning Logistic | inner ACC 84.62% | outer ACC 83.70%\n",
      "outer fold 4/5 | tuning RandomForest | inner ACC 85.75% | outer ACC 84.50%\n",
      "outer fold 5/5 | tuning KNN      | inner ACC 83.12% | outer ACC 81.30%\n",
      "outer fold 5/5 | tuning Logistic | inner ACC 84.55% | outer ACC 85.10%\n",
      "outer fold 5/5 | tuning RandomForest | inner ACC 85.45% | outer ACC 85.20%\n",
      "CPU times: user 49.1 s, sys: 4.63 s, total: 53.7 s\n",
      "Wall time: 1h 2min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# ^^ this handy Jupyter magic times the execution of the cell for you\n",
    "\n",
    "cv_scores = {name: [] for name, gs_est in gridcvs.items()}\n",
    "\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# The outer loop for algorithm selection\n",
    "c = 1\n",
    "for outer_train_idx, outer_valid_idx in skfold.split(X_train,y_train):\n",
    "    for name, gs_est in sorted(gridcvs.items()):\n",
    "        print('outer fold %d/5 | tuning %-8s' % (c, name), end='')\n",
    "\n",
    "        # The inner loop for hyperparameter tuning\n",
    "        gs_est.fit(X_train.iloc[outer_train_idx], y_train.iloc[outer_train_idx])\n",
    "        y_pred = gs_est.predict(X_train.iloc[outer_valid_idx])\n",
    "        acc = accuracy_score(y_true=y_train.iloc[outer_valid_idx], y_pred=y_pred)\n",
    "        print(' | inner ACC %.2f%% | outer ACC %.2f%%' %\n",
    "              (gs_est.best_score_ * 100, acc * 100))\n",
    "        cv_scores[name].append(acc)\n",
    "\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN      | outer CV acc. 82.36% +\\- 0.618\n",
      "RandomForest | outer CV acc. 85.06% +\\- 0.476\n",
      "Logistic | outer CV acc. 84.70% +\\- 0.576\n",
      "\n",
      "KNN best parameters {'classifier__n_neighbors': 20, 'classifier__weights': 'distance'}\n",
      "RandomForest best parameters {'classifier__max_features': 20}\n",
      "Logistic best parameters {'classifier__C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Looking at the results\n",
    "for name in cv_scores:\n",
    "    print('%-8s | outer CV acc. %.2f%% +\\- %.3f' % (\n",
    "          name, 100 * np.mean(cv_scores[name]), 100 * np.std(cv_scores[name])))\n",
    "print()\n",
    "for name in cv_scores:\n",
    "    print('{} best parameters'.format(name), gridcvs[name].best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 83.06% (average over CV test folds)\n",
      "Best Parameters: {'classifier__n_neighbors': 60, 'classifier__weights': 'distance'}\n",
      "Training Accuracy: 100.00%\n",
      "Test Accuracy: 82.30%\n",
      "\n",
      "Accuracy 85.64% (average over CV test folds)\n",
      "Best Parameters: {'classifier__max_features': 20}\n",
      "Training Accuracy: 100.00%\n",
      "Test Accuracy: 84.38%\n",
      "\n",
      "Accuracy 84.78% (average over CV test folds)\n",
      "Best Parameters: {'classifier__C': 0.1}\n",
      "Training Accuracy: 85.48%\n",
      "Test Accuracy: 84.33%\n"
     ]
    }
   ],
   "source": [
    "# Fitting a model to the whole training set\n",
    "# using the \"best\" KNN algorithm\n",
    "best_algo = gridcvs['KNN']\n",
    "\n",
    "best_algo.fit(X_train, y_train)\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "print('Accuracy %.2f%% (average over CV test folds)' %\n",
    "      (100 * best_algo.best_score_))\n",
    "print('Best Parameters: %s' % gridcvs['KNN'].best_params_)\n",
    "print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "print('Test Accuracy: %.2f%%' % (100 * test_acc))\n",
    "knn_accuracy.append(test_acc)\n",
    "knn_accuracy_train.append(train_acc)\n",
    "\n",
    "# using the \"best\" RandomForest algorithm\n",
    "best_algo = gridcvs['RandomForest']\n",
    "\n",
    "best_algo.fit(X_train, y_train)\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "print('\\nAccuracy %.2f%% (average over CV test folds)' %\n",
    "      (100 * best_algo.best_score_))\n",
    "print('Best Parameters: %s' % gridcvs['RandomForest'].best_params_)\n",
    "print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "print('Test Accuracy: %.2f%%' % (100 * test_acc))\n",
    "randforest_accuracy.append(test_acc)\n",
    "randforest_accuracy_train.append(train_acc)\n",
    "\n",
    "# using the \"best\" Logistic algorithm\n",
    "best_algo = gridcvs['Logistic']\n",
    "\n",
    "best_algo.fit(X_train, y_train)\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "\n",
    "print('\\nAccuracy %.2f%% (average over CV test folds)' %\n",
    "      (100 * best_algo.best_score_))\n",
    "print('Best Parameters: %s' % gridcvs['Logistic'].best_params_)\n",
    "print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "print('Test Accuracy: %.2f%%' % (100 * test_acc))\n",
    "logreg_accuracy.append(test_acc)\n",
    "logreg_accuracy_train.append(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8230267864239726\n",
      "0.8438120976075034\n",
      "0.8433351879818775\n"
     ]
    }
   ],
   "source": [
    "print(knn_accuracy[1])\n",
    "print(randforest_accuracy[1])\n",
    "print(logreg_accuracy[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# take all our penguin data, and reserve 20% of it for testing \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_p, y_p,\n",
    "                                                    train_size=5000,\n",
    "                                                    random_state=13245,\n",
    "                                                    stratify=y_p)\n",
    "\n",
    "\n",
    "# Initializing Classifiers\n",
    "clf1 = KNeighborsClassifier()\n",
    "clf2 = RandomForestClassifier(n_estimators = 1024)\n",
    "clf3 = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Declaring parameters\n",
    "K_list = np.array([n*20 for n in range(1,26)])\n",
    "F_list = np.array([1, 2, 4, 6, 8, 12, 16, 20])\n",
    "C_list = np.array([10**(-8), 10**(-7), 10**(-6), 10**(-5), 10**(-4), 10**(-3), 10**(-2), 10**(-1), \n",
    "                       10**(0), 10**(1), 10**(2), 10**(3), 10**(4)])\n",
    "\n",
    "# Building the pipelines\n",
    "pipe1 = Pipeline([('std', StandardScaler()),\n",
    "                  ('classifier', clf1)])\n",
    "\n",
    "pipe2 = Pipeline([('std', StandardScaler()),\n",
    "                  ('classifier', clf2)])\n",
    "\n",
    "pipe3 = Pipeline([('std', StandardScaler()),\n",
    "                  ('classifier', clf3)])\n",
    "\n",
    "\n",
    "# Setting up the parameter grids\n",
    "param_grid1 = [{'classifier__weights': ['uniform', 'distance'],\n",
    "                'classifier__n_neighbors': K_list}]\n",
    "\n",
    "param_grid2 = [{'classifier__max_features': F_list}]\n",
    "\n",
    "param_grid3 = [{'classifier__C': C_list}]\n",
    "\n",
    "\n",
    "# Setting up multiple GridSearchCV objects, 1 for each algorithm\n",
    "gridcvs = {}\n",
    "\n",
    "for pgrid, est, name in zip((param_grid1, param_grid2, param_grid3),\n",
    "                            (pipe1, pipe2, pipe3),\n",
    "                            ('KNN', 'RandomForest', 'Logistic')):\n",
    "    gcv = GridSearchCV(estimator=est,\n",
    "                       param_grid=pgrid,\n",
    "                       scoring='accuracy',\n",
    "                       n_jobs=3,\n",
    "                       cv=5, # just 2-fold inner loop, i.e. train/test\n",
    "                       verbose=0,\n",
    "                       return_train_score=True,\n",
    "                       refit='accuracy')\n",
    "    gridcvs[name] = gcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer fold 1/5 | tuning KNN      | inner ACC 81.95% | outer ACC 80.80%\n",
      "outer fold 1/5 | tuning Logistic | inner ACC 83.20% | outer ACC 84.20%\n",
      "outer fold 1/5 | tuning RandomForest | inner ACC 84.20% | outer ACC 84.10%\n",
      "outer fold 2/5 | tuning KNN      | inner ACC 81.73% | outer ACC 82.60%\n",
      "outer fold 2/5 | tuning Logistic | inner ACC 83.10% | outer ACC 83.60%\n",
      "outer fold 2/5 | tuning RandomForest | inner ACC 84.08% | outer ACC 84.30%\n",
      "outer fold 3/5 | tuning KNN      | inner ACC 81.62% | outer ACC 83.40%\n",
      "outer fold 3/5 | tuning Logistic | inner ACC 83.12% | outer ACC 84.60%\n",
      "outer fold 3/5 | tuning RandomForest | inner ACC 83.45% | outer ACC 85.00%\n",
      "outer fold 4/5 | tuning KNN      | inner ACC 81.95% | outer ACC 80.60%\n",
      "outer fold 4/5 | tuning Logistic | inner ACC 83.47% | outer ACC 82.70%\n",
      "outer fold 4/5 | tuning RandomForest | inner ACC 84.17% | outer ACC 84.10%\n",
      "outer fold 5/5 | tuning KNN      | inner ACC 82.05% | outer ACC 81.60%\n",
      "outer fold 5/5 | tuning Logistic | inner ACC 83.38% | outer ACC 82.80%\n",
      "outer fold 5/5 | tuning RandomForest | inner ACC 83.90% | outer ACC 83.60%\n",
      "CPU times: user 49.2 s, sys: 4.99 s, total: 54.1 s\n",
      "Wall time: 1h 2min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# ^^ this handy Jupyter magic times the execution of the cell for you\n",
    "\n",
    "cv_scores = {name: [] for name, gs_est in gridcvs.items()}\n",
    "\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# The outer loop for algorithm selection\n",
    "c = 1\n",
    "for outer_train_idx, outer_valid_idx in skfold.split(X_train,y_train):\n",
    "    for name, gs_est in sorted(gridcvs.items()):\n",
    "        print('outer fold %d/5 | tuning %-8s' % (c, name), end='')\n",
    "\n",
    "        # The inner loop for hyperparameter tuning\n",
    "        gs_est.fit(X_train.iloc[outer_train_idx], y_train.iloc[outer_train_idx])\n",
    "        y_pred = gs_est.predict(X_train.iloc[outer_valid_idx])\n",
    "        acc = accuracy_score(y_true=y_train.iloc[outer_valid_idx], y_pred=y_pred)\n",
    "        print(' | inner ACC %.2f%% | outer ACC %.2f%%' %\n",
    "              (gs_est.best_score_ * 100, acc * 100))\n",
    "        cv_scores[name].append(acc)\n",
    "\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN      | outer CV acc. 81.80% +\\- 1.066\n",
      "RandomForest | outer CV acc. 84.22% +\\- 0.453\n",
      "Logistic | outer CV acc. 83.58% +\\- 0.749\n",
      "\n",
      "KNN best parameters {'classifier__n_neighbors': 60, 'classifier__weights': 'distance'}\n",
      "RandomForest best parameters {'classifier__max_features': 8}\n",
      "Logistic best parameters {'classifier__C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Looking at the results\n",
    "for name in cv_scores:\n",
    "    print('%-8s | outer CV acc. %.2f%% +\\- %.3f' % (\n",
    "          name, 100 * np.mean(cv_scores[name]), 100 * np.std(cv_scores[name])))\n",
    "print()\n",
    "for name in cv_scores:\n",
    "    print('{} best parameters'.format(name), gridcvs[name].best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 82.06% (average over CV test folds)\n",
      "Best Parameters: {'classifier__n_neighbors': 40, 'classifier__weights': 'uniform'}\n",
      "Training Accuracy: 82.80%\n",
      "Test Accuracy: 81.86%\n",
      "\n",
      "Accuracy 83.82% (average over CV test folds)\n",
      "Best Parameters: {'classifier__max_features': 12}\n",
      "Training Accuracy: 100.00%\n",
      "Test Accuracy: 84.72%\n",
      "\n",
      "Accuracy 83.38% (average over CV test folds)\n",
      "Best Parameters: {'classifier__C': 1.0}\n",
      "Training Accuracy: 84.20%\n",
      "Test Accuracy: 84.64%\n"
     ]
    }
   ],
   "source": [
    "# Fitting a model to the whole training set\n",
    "# using the \"best\" KNN algorithm\n",
    "best_algo = gridcvs['KNN']\n",
    "\n",
    "best_algo.fit(X_train, y_train)\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "print('Accuracy %.2f%% (average over CV test folds)' %\n",
    "      (100 * best_algo.best_score_))\n",
    "print('Best Parameters: %s' % gridcvs['KNN'].best_params_)\n",
    "print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "print('Test Accuracy: %.2f%%' % (100 * test_acc))\n",
    "knn_accuracy.append(test_acc)\n",
    "knn_accuracy_train.append(train_acc)\n",
    "\n",
    "# using the \"best\" RandomForest algorithm\n",
    "best_algo = gridcvs['RandomForest']\n",
    "\n",
    "best_algo.fit(X_train, y_train)\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "print('\\nAccuracy %.2f%% (average over CV test folds)' %\n",
    "      (100 * best_algo.best_score_))\n",
    "print('Best Parameters: %s' % gridcvs['RandomForest'].best_params_)\n",
    "print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "print('Test Accuracy: %.2f%%' % (100 * test_acc))\n",
    "randforest_accuracy.append(test_acc)\n",
    "randforest_accuracy_train.append(train_acc)\n",
    "\n",
    "# using the \"best\" Logistic algorithm\n",
    "best_algo = gridcvs['Logistic']\n",
    "\n",
    "best_algo.fit(X_train, y_train)\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "\n",
    "print('\\nAccuracy %.2f%% (average over CV test folds)' %\n",
    "      (100 * best_algo.best_score_))\n",
    "print('Best Parameters: %s' % gridcvs['Logistic'].best_params_)\n",
    "print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "print('Test Accuracy: %.2f%%' % (100 * test_acc))\n",
    "logreg_accuracy.append(test_acc)\n",
    "logreg_accuracy_train.append(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8185756299181305\n",
      "0.8471902074556872\n",
      "0.846395358079644\n"
     ]
    }
   ],
   "source": [
    "print(knn_accuracy[2])\n",
    "print(randforest_accuracy[2])\n",
    "print(logreg_accuracy[2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average KNN Test Accuracy:  0.9426666666666667\n",
      "Average Random Forest Test Accuracy:  1.0\n",
      "Average Logistic Regression Test Accuracy:  0.8498666666666667\n"
     ]
    }
   ],
   "source": [
    "#report average train accuracy per classifier (with best parameter)\n",
    "print(\"Average KNN Test Accuracy: \", np.mean(knn_accuracy_train))\n",
    "print(\"Average Random Forest Test Accuracy: \", np.mean(randforest_accuracy_train))\n",
    "print(\"Average Logistic Regression Test Accuracy: \", np.mean(logreg_accuracy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average KNN Test Accuracy:  0.8217682749119043\n",
      "Average Random Forest Test Accuracy:  0.8470444850700792\n",
      "Average Logistic Regression Test Accuracy:  0.8440637999099171\n"
     ]
    }
   ],
   "source": [
    "#report average test accuracy per classifier (with best parameter)\n",
    "print(\"Average KNN Test Accuracy: \", np.mean(knn_accuracy))\n",
    "print(\"Average Random Forest Test Accuracy: \", np.mean(randforest_accuracy))\n",
    "print(\"Average Logistic Regression Test Accuracy: \", np.mean(logreg_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T - test for ADULT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-test between the different algorithms\n",
    "\n",
    "knn_forest = stats.ttest_ind(knn_accuracy, randforest_accuracy)\n",
    "\n",
    "forest_logistic = stats.ttest_ind(randforest_accuracy, logreg_accuracy)\n",
    "\n",
    "logistic_knn = stats.ttest_ind(logreg_accuracy, knn_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN and RandomForest:  Ttest_indResult(statistic=-10.389223178576424, pvalue=0.00048468528084998807)\n",
      "\n",
      "RandomForest and Logistic Regression:  Ttest_indResult(statistic=1.3668281006611518, pvalue=0.2434617939743712)\n",
      "\n",
      "Logistic Regression and KNN:  Ttest_indResult(statistic=11.135200471769094, pvalue=0.00037013662334016575)\n"
     ]
    }
   ],
   "source": [
    "# Results of the T-tests\n",
    "\n",
    "print('KNN and RandomForest: ', knn_forest)\n",
    "print('\\nRandomForest and Logistic Regression: ', forest_logistic)\n",
    "print('\\nLogistic Regression and KNN: ', logistic_knn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
