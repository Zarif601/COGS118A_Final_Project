{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pandas import read_csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['lettr', 'x-box', 'y-box', 'width', 'high', 'onpix', 'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar', 'x2ybr', 'xy2br',\n",
    "           'x-ege', 'xegvy', 'y-ege', 'yegvx']\n",
    "      \n",
    "df = pd.read_csv('letter-recognition.data', header = None, names = headers, na_values = ' ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the labels with 0's and 1's\n",
    "for_numeric = ['A','B','C','D','E','F','G','H','I','J','K','L','M']\n",
    "\n",
    "numeric = df['lettr'].tolist()\n",
    "\n",
    "for i, j in enumerate(numeric):\n",
    "    if j in for_numeric :\n",
    "        numeric[i] = 1\n",
    "    else:\n",
    "        numeric[i] = 0\n",
    "        \n",
    "df['lettr'] = numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating features and labels\n",
    "X_p = df.iloc[:, 1:]\n",
    "y_p = df.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy vectors for train set\n",
    "logreg_accuracy_train = []\n",
    "randforest_accuracy_train = []\n",
    "knn_accuracy_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy vectors for test set\n",
    "logreg_accuracy = []\n",
    "randforest_accuracy = []\n",
    "knn_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21 µs, sys: 9 µs, total: 30 µs\n",
      "Wall time: 35.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# take all our penguin data, and reserve 20% of it for testing \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_p, y_p,\n",
    "                                                    train_size=5000,\n",
    "                                                    random_state=12345,\n",
    "                                                    stratify=y_p)\n",
    "\n",
    "\n",
    "# Initializing Classifiers\n",
    "clf1 = KNeighborsClassifier()\n",
    "clf2 = RandomForestClassifier(n_estimators = 1024)\n",
    "clf3 = LogisticRegression(solver='liblinear', multi_class='auto')\n",
    "\n",
    "# Declaring parameters\n",
    "K_list = np.array([n*20 for n in range(1,26)])\n",
    "F_list = np.array([1, 2, 4, 6, 8, 12, 16])\n",
    "C_list = np.array([10**(-8), 10**(-7), 10**(-6), 10**(-5), 10**(-4), 10**(-3), 10**(-2), 10**(-1), \n",
    "                       10**(0), 10**(1), 10**(2), 10**(3), 10**(4)])\n",
    "\n",
    "# Building the pipelines\n",
    "pipe1 = Pipeline([('std', StandardScaler()),\n",
    "                  ('classifier', clf1)])\n",
    "\n",
    "pipe2 = Pipeline([('std', StandardScaler()),\n",
    "                  ('classifier', clf2)])\n",
    "\n",
    "pipe3 = Pipeline([('std', StandardScaler()),\n",
    "                  ('classifier', clf3)])\n",
    "\n",
    "\n",
    "# Setting up the parameter grids\n",
    "param_grid1 = [{'classifier__weights': ['uniform', 'distance'],\n",
    "                'classifier__n_neighbors': K_list}]\n",
    "\n",
    "param_grid2 = [{'classifier__max_features': F_list}]\n",
    "\n",
    "param_grid3 = [{'classifier__C': C_list}]\n",
    "\n",
    "\n",
    "# Setting up multiple GridSearchCV objects, 1 for each algorithm\n",
    "gridcvs = {}\n",
    "\n",
    "for pgrid, est, name in zip((param_grid1, param_grid2, param_grid3),\n",
    "                            (pipe1, pipe2, pipe3),\n",
    "                            ('KNN', 'RandomForest', 'Logistic')):\n",
    "    gcv = GridSearchCV(estimator=est,\n",
    "                       param_grid=pgrid,\n",
    "                       scoring='accuracy',\n",
    "                       n_jobs=3,\n",
    "                       cv=5, # just 2-fold inner loop, i.e. train/test\n",
    "                       verbose=0,\n",
    "                       return_train_score=True,\n",
    "                       refit='accuracy')\n",
    "    gridcvs[name] = gcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer fold 1/5 | tuning KNN      | inner ACC 89.90% | outer ACC 91.00%\n",
      "outer fold 1/5 | tuning Logistic | inner ACC 72.72% | outer ACC 69.70%\n",
      "outer fold 1/5 | tuning RandomForest | inner ACC 92.60% | outer ACC 93.50%\n",
      "outer fold 2/5 | tuning KNN      | inner ACC 90.18% | outer ACC 89.10%\n",
      "outer fold 2/5 | tuning Logistic | inner ACC 72.40% | outer ACC 72.90%\n",
      "outer fold 2/5 | tuning RandomForest | inner ACC 92.97% | outer ACC 92.20%\n",
      "outer fold 3/5 | tuning KNN      | inner ACC 89.83% | outer ACC 91.20%\n",
      "outer fold 3/5 | tuning Logistic | inner ACC 72.20% | outer ACC 74.20%\n",
      "outer fold 3/5 | tuning RandomForest | inner ACC 92.97% | outer ACC 94.60%\n",
      "outer fold 4/5 | tuning KNN      | inner ACC 89.03% | outer ACC 91.90%\n",
      "outer fold 4/5 | tuning Logistic | inner ACC 72.17% | outer ACC 74.50%\n",
      "outer fold 4/5 | tuning RandomForest | inner ACC 92.58% | outer ACC 93.90%\n",
      "outer fold 5/5 | tuning KNN      | inner ACC 89.53% | outer ACC 89.00%\n",
      "outer fold 5/5 | tuning Logistic | inner ACC 72.75% | outer ACC 71.20%\n",
      "outer fold 5/5 | tuning RandomForest | inner ACC 93.23% | outer ACC 92.90%\n",
      "CPU times: user 43.8 s, sys: 4.15 s, total: 47.9 s\n",
      "Wall time: 27min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# ^^ this handy Jupyter magic times the execution of the cell for you\n",
    "\n",
    "cv_scores = {name: [] for name, gs_est in gridcvs.items()}\n",
    "\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# The outer loop for algorithm selection\n",
    "c = 1\n",
    "for outer_train_idx, outer_valid_idx in skfold.split(X_train,y_train):\n",
    "    for name, gs_est in sorted(gridcvs.items()):\n",
    "        print('outer fold %d/5 | tuning %-8s' % (c, name), end='')\n",
    "\n",
    "        # The inner loop for hyperparameter tuning\n",
    "        gs_est.fit(X_train.iloc[outer_train_idx], y_train.iloc[outer_train_idx])\n",
    "        y_pred = gs_est.predict(X_train.iloc[outer_valid_idx])\n",
    "        acc = accuracy_score(y_true=y_train.iloc[outer_valid_idx], y_pred=y_pred)\n",
    "        print(' | inner ACC %.2f%% | outer ACC %.2f%%' %\n",
    "              (gs_est.best_score_ * 100, acc * 100))\n",
    "        cv_scores[name].append(acc)\n",
    "\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN      | outer CV acc. 90.44% +\\- 1.174\n",
      "RandomForest | outer CV acc. 93.42% +\\- 0.823\n",
      "Logistic | outer CV acc. 72.50% +\\- 1.821\n",
      "\n",
      "KNN best parameters {'classifier__n_neighbors': 20, 'classifier__weights': 'distance'}\n",
      "RandomForest best parameters {'classifier__max_features': 4}\n",
      "Logistic best parameters {'classifier__C': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Looking at the results\n",
    "for name in cv_scores:\n",
    "    print('%-8s | outer CV acc. %.2f%% +\\- %.3f' % (\n",
    "          name, 100 * np.mean(cv_scores[name]), 100 * np.std(cv_scores[name])))\n",
    "print()\n",
    "for name in cv_scores:\n",
    "    print('{} best parameters'.format(name), gridcvs[name].best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 91.02% (average over CV test folds)\n",
      "Best Parameters: {'classifier__n_neighbors': 20, 'classifier__weights': 'distance'}\n",
      "Training Accuracy: 100.00%\n",
      "Test Accuracy: 92.83%\n",
      "\n",
      "Accuracy 93.74% (average over CV test folds)\n",
      "Best Parameters: {'classifier__max_features': 8}\n",
      "Training Accuracy: 100.00%\n",
      "Test Accuracy: 94.92%\n",
      "\n",
      "Accuracy 72.48% (average over CV test folds)\n",
      "Best Parameters: {'classifier__C': 0.1}\n",
      "Training Accuracy: 72.30%\n",
      "Test Accuracy: 72.59%\n"
     ]
    }
   ],
   "source": [
    "# Fitting a model to the whole training set\n",
    "# using the \"best\" KNN algorithm\n",
    "best_algo = gridcvs['KNN']\n",
    "\n",
    "best_algo.fit(X_train, y_train)\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "print('Accuracy %.2f%% (average over CV test folds)' %\n",
    "      (100 * best_algo.best_score_))\n",
    "print('Best Parameters: %s' % gridcvs['KNN'].best_params_)\n",
    "print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "print('Test Accuracy: %.2f%%' % (100 * test_acc))\n",
    "knn_accuracy.append(test_acc)\n",
    "knn_accuracy_train.append(train_acc)\n",
    "\n",
    "# using the \"best\" RandomForest algorithm\n",
    "best_algo = gridcvs['RandomForest']\n",
    "\n",
    "best_algo.fit(X_train, y_train)\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "print('\\nAccuracy %.2f%% (average over CV test folds)' %\n",
    "      (100 * best_algo.best_score_))\n",
    "print('Best Parameters: %s' % gridcvs['RandomForest'].best_params_)\n",
    "print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "print('Test Accuracy: %.2f%%' % (100 * test_acc))\n",
    "randforest_accuracy.append(test_acc)\n",
    "randforest_accuracy_train.append(train_acc)\n",
    "\n",
    "# using the \"best\" Logistic algorithm\n",
    "best_algo = gridcvs['Logistic']\n",
    "\n",
    "best_algo.fit(X_train, y_train)\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "\n",
    "print('\\nAccuracy %.2f%% (average over CV test folds)' %\n",
    "      (100 * best_algo.best_score_))\n",
    "print('Best Parameters: %s' % gridcvs['Logistic'].best_params_)\n",
    "print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "print('Test Accuracy: %.2f%%' % (100 * test_acc))\n",
    "logreg_accuracy.append(test_acc)\n",
    "logreg_accuracy_train.append(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9283333333333333\n",
      "0.9492\n",
      "0.7258666666666667\n"
     ]
    }
   ],
   "source": [
    "print(knn_accuracy[0])\n",
    "print(randforest_accuracy[0])\n",
    "print(logreg_accuracy[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# take all our penguin data, and reserve 20% of it for testing \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_p, y_p,\n",
    "                                                    train_size=5000,\n",
    "                                                    random_state=5432,\n",
    "                                                    stratify=y_p)\n",
    "\n",
    "\n",
    "# Initializing Classifiers\n",
    "clf1 = KNeighborsClassifier()\n",
    "clf2 = RandomForestClassifier(n_estimators = 1024)\n",
    "clf3 = LogisticRegression(solver='liblinear', multi_class='auto')\n",
    "\n",
    "# Declaring parameters\n",
    "K_list = np.array([n*20 for n in range(1,26)])\n",
    "F_list = np.array([1, 2, 4, 6, 8, 12, 16])\n",
    "C_list = np.array([10**(-8), 10**(-7), 10**(-6), 10**(-5), 10**(-4), 10**(-3), 10**(-2), 10**(-1), \n",
    "                       10**(0), 10**(1), 10**(2), 10**(3), 10**(4)])\n",
    "\n",
    "# Building the pipelines\n",
    "pipe1 = Pipeline([('std', StandardScaler()),\n",
    "                  ('classifier', clf1)])\n",
    "\n",
    "pipe2 = Pipeline([('std', StandardScaler()),\n",
    "                  ('classifier', clf2)])\n",
    "\n",
    "pipe3 = Pipeline([('std', StandardScaler()),\n",
    "                  ('classifier', clf3)])\n",
    "\n",
    "\n",
    "# Setting up the parameter grids\n",
    "param_grid1 = [{'classifier__weights': ['uniform', 'distance'],\n",
    "                'classifier__n_neighbors': K_list}]\n",
    "\n",
    "param_grid2 = [{'classifier__max_features': F_list}]\n",
    "\n",
    "param_grid3 = [{'classifier__C': C_list}]\n",
    "\n",
    "\n",
    "# Setting up multiple GridSearchCV objects, 1 for each algorithm\n",
    "gridcvs = {}\n",
    "\n",
    "for pgrid, est, name in zip((param_grid1, param_grid2, param_grid3),\n",
    "                            (pipe1, pipe2, pipe3),\n",
    "                            ('KNN', 'RandomForest', 'Logistic')):\n",
    "    gcv = GridSearchCV(estimator=est,\n",
    "                       param_grid=pgrid,\n",
    "                       scoring='accuracy',\n",
    "                       n_jobs=3,\n",
    "                       cv=5, # just 2-fold inner loop, i.e. train/test\n",
    "                       verbose=0,\n",
    "                       return_train_score=True,\n",
    "                       refit='accuracy')\n",
    "    gridcvs[name] = gcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer fold 1/5 | tuning KNN      | inner ACC 90.12% | outer ACC 92.90%\n",
      "outer fold 1/5 | tuning Logistic | inner ACC 73.10% | outer ACC 72.30%\n",
      "outer fold 1/5 | tuning RandomForest | inner ACC 93.40% | outer ACC 95.70%\n",
      "outer fold 2/5 | tuning KNN      | inner ACC 90.00% | outer ACC 93.00%\n",
      "outer fold 2/5 | tuning Logistic | inner ACC 72.52% | outer ACC 74.70%\n",
      "outer fold 2/5 | tuning RandomForest | inner ACC 93.33% | outer ACC 93.90%\n",
      "outer fold 3/5 | tuning KNN      | inner ACC 90.70% | outer ACC 91.20%\n",
      "outer fold 3/5 | tuning Logistic | inner ACC 72.85% | outer ACC 73.10%\n",
      "outer fold 3/5 | tuning RandomForest | inner ACC 93.50% | outer ACC 93.60%\n",
      "outer fold 4/5 | tuning KNN      | inner ACC 90.10% | outer ACC 90.20%\n",
      "outer fold 4/5 | tuning Logistic | inner ACC 73.85% | outer ACC 70.10%\n",
      "outer fold 4/5 | tuning RandomForest | inner ACC 93.50% | outer ACC 92.70%\n",
      "outer fold 5/5 | tuning KNN      | inner ACC 90.60% | outer ACC 89.50%\n",
      "outer fold 5/5 | tuning Logistic | inner ACC 72.58% | outer ACC 74.90%\n",
      "outer fold 5/5 | tuning RandomForest | inner ACC 93.08% | outer ACC 92.90%\n",
      "CPU times: user 46.5 s, sys: 4.02 s, total: 50.5 s\n",
      "Wall time: 29min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# ^^ this handy Jupyter magic times the execution of the cell for you\n",
    "\n",
    "cv_scores = {name: [] for name, gs_est in gridcvs.items()}\n",
    "\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# The outer loop for algorithm selection\n",
    "c = 1\n",
    "for outer_train_idx, outer_valid_idx in skfold.split(X_train,y_train):\n",
    "    for name, gs_est in sorted(gridcvs.items()):\n",
    "        print('outer fold %d/5 | tuning %-8s' % (c, name), end='')\n",
    "\n",
    "        # The inner loop for hyperparameter tuning\n",
    "        gs_est.fit(X_train.iloc[outer_train_idx], y_train.iloc[outer_train_idx])\n",
    "        y_pred = gs_est.predict(X_train.iloc[outer_valid_idx])\n",
    "        acc = accuracy_score(y_true=y_train.iloc[outer_valid_idx], y_pred=y_pred)\n",
    "        print(' | inner ACC %.2f%% | outer ACC %.2f%%' %\n",
    "              (gs_est.best_score_ * 100, acc * 100))\n",
    "        cv_scores[name].append(acc)\n",
    "\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN      | outer CV acc. 91.36% +\\- 1.407\n",
      "RandomForest | outer CV acc. 93.76% +\\- 1.065\n",
      "Logistic | outer CV acc. 73.02% +\\- 1.755\n",
      "\n",
      "KNN best parameters {'classifier__n_neighbors': 20, 'classifier__weights': 'distance'}\n",
      "RandomForest best parameters {'classifier__max_features': 6}\n",
      "Logistic best parameters {'classifier__C': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Looking at the results\n",
    "for name in cv_scores:\n",
    "    print('%-8s | outer CV acc. %.2f%% +\\- %.3f' % (\n",
    "          name, 100 * np.mean(cv_scores[name]), 100 * np.std(cv_scores[name])))\n",
    "print()\n",
    "for name in cv_scores:\n",
    "    print('{} best parameters'.format(name), gridcvs[name].best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 91.56% (average over CV test folds)\n",
      "Best Parameters: {'classifier__n_neighbors': 20, 'classifier__weights': 'distance'}\n",
      "Training Accuracy: 100.00%\n",
      "Test Accuracy: 93.19%\n",
      "\n",
      "Accuracy 94.06% (average over CV test folds)\n",
      "Best Parameters: {'classifier__max_features': 4}\n",
      "Training Accuracy: 100.00%\n",
      "Test Accuracy: 94.85%\n",
      "\n",
      "Accuracy 72.84% (average over CV test folds)\n",
      "Best Parameters: {'classifier__C': 10.0}\n",
      "Training Accuracy: 73.06%\n",
      "Test Accuracy: 72.37%\n"
     ]
    }
   ],
   "source": [
    "# Fitting a model to the whole training set\n",
    "# using the \"best\" KNN algorithm\n",
    "best_algo = gridcvs['KNN']\n",
    "\n",
    "best_algo.fit(X_train, y_train)\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "print('Accuracy %.2f%% (average over CV test folds)' %\n",
    "      (100 * best_algo.best_score_))\n",
    "print('Best Parameters: %s' % gridcvs['KNN'].best_params_)\n",
    "print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "print('Test Accuracy: %.2f%%' % (100 * test_acc))\n",
    "knn_accuracy.append(test_acc)\n",
    "knn_accuracy_train.append(train_acc)\n",
    "\n",
    "# using the \"best\" RandomForest algorithm\n",
    "best_algo = gridcvs['RandomForest']\n",
    "\n",
    "best_algo.fit(X_train, y_train)\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "print('\\nAccuracy %.2f%% (average over CV test folds)' %\n",
    "      (100 * best_algo.best_score_))\n",
    "print('Best Parameters: %s' % gridcvs['RandomForest'].best_params_)\n",
    "print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "print('Test Accuracy: %.2f%%' % (100 * test_acc))\n",
    "randforest_accuracy.append(test_acc)\n",
    "randforest_accuracy_train.append(train_acc)\n",
    "\n",
    "# using the \"best\" Logistic algorithm\n",
    "best_algo = gridcvs['Logistic']\n",
    "\n",
    "best_algo.fit(X_train, y_train)\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "\n",
    "print('\\nAccuracy %.2f%% (average over CV test folds)' %\n",
    "      (100 * best_algo.best_score_))\n",
    "print('Best Parameters: %s' % gridcvs['Logistic'].best_params_)\n",
    "print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "print('Test Accuracy: %.2f%%' % (100 * test_acc))\n",
    "logreg_accuracy.append(test_acc)\n",
    "logreg_accuracy_train.append(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9318666666666666\n",
      "0.9485333333333333\n",
      "0.7236666666666667\n"
     ]
    }
   ],
   "source": [
    "print(knn_accuracy[1])\n",
    "print(randforest_accuracy[1])\n",
    "print(logreg_accuracy[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# take all our penguin data, and reserve 20% of it for testing \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_p, y_p,\n",
    "                                                    train_size=5000,\n",
    "                                                    random_state=13245,\n",
    "                                                    stratify=y_p)\n",
    "\n",
    "\n",
    "# Initializing Classifiers\n",
    "clf1 = KNeighborsClassifier()\n",
    "clf2 = RandomForestClassifier(n_estimators = 1024)\n",
    "clf3 = LogisticRegression(solver='liblinear', multi_class='auto')\n",
    "\n",
    "# Declaring parameters\n",
    "K_list = np.array([n*20 for n in range(1,26)])\n",
    "F_list = np.array([1, 2, 4, 6, 8, 12, 16])\n",
    "C_list = np.array([10**(-8), 10**(-7), 10**(-6), 10**(-5), 10**(-4), 10**(-3), 10**(-2), 10**(-1), \n",
    "                       10**(0), 10**(1), 10**(2), 10**(3), 10**(4)])\n",
    "\n",
    "# Building the pipelines\n",
    "pipe1 = Pipeline([('std', StandardScaler()),\n",
    "                  ('classifier', clf1)])\n",
    "\n",
    "pipe2 = Pipeline([('std', StandardScaler()),\n",
    "                  ('classifier', clf2)])\n",
    "\n",
    "pipe3 = Pipeline([('std', StandardScaler()),\n",
    "                  ('classifier', clf3)])\n",
    "\n",
    "\n",
    "# Setting up the parameter grids\n",
    "param_grid1 = [{'classifier__weights': ['uniform', 'distance'],\n",
    "                'classifier__n_neighbors': K_list}]\n",
    "\n",
    "param_grid2 = [{'classifier__max_features': F_list}]\n",
    "\n",
    "param_grid3 = [{'classifier__C': C_list}]\n",
    "\n",
    "\n",
    "# Setting up multiple GridSearchCV objects, 1 for each algorithm\n",
    "gridcvs = {}\n",
    "\n",
    "for pgrid, est, name in zip((param_grid1, param_grid2, param_grid3),\n",
    "                            (pipe1, pipe2, pipe3),\n",
    "                            ('KNN', 'RandomForest', 'Logistic')):\n",
    "    gcv = GridSearchCV(estimator=est,\n",
    "                       param_grid=pgrid,\n",
    "                       scoring='accuracy',\n",
    "                       n_jobs=3,\n",
    "                       cv=5, # just 2-fold inner loop, i.e. train/test\n",
    "                       verbose=0,\n",
    "                       return_train_score=True,\n",
    "                       refit='accuracy')\n",
    "    gridcvs[name] = gcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer fold 1/5 | tuning KNN      | inner ACC 90.62% | outer ACC 91.20%\n",
      "outer fold 1/5 | tuning Logistic | inner ACC 72.62% | outer ACC 69.30%\n",
      "outer fold 1/5 | tuning RandomForest | inner ACC 93.77% | outer ACC 94.50%\n",
      "outer fold 2/5 | tuning KNN      | inner ACC 90.53% | outer ACC 92.60%\n",
      "outer fold 2/5 | tuning Logistic | inner ACC 71.75% | outer ACC 71.70%\n",
      "outer fold 2/5 | tuning RandomForest | inner ACC 93.40% | outer ACC 94.70%\n",
      "outer fold 3/5 | tuning KNN      | inner ACC 90.22% | outer ACC 91.10%\n",
      "outer fold 3/5 | tuning Logistic | inner ACC 71.95% | outer ACC 73.20%\n",
      "outer fold 3/5 | tuning RandomForest | inner ACC 93.42% | outer ACC 95.30%\n",
      "outer fold 4/5 | tuning KNN      | inner ACC 90.50% | outer ACC 90.70%\n",
      "outer fold 4/5 | tuning Logistic | inner ACC 72.28% | outer ACC 73.80%\n",
      "outer fold 4/5 | tuning RandomForest | inner ACC 93.73% | outer ACC 92.70%\n",
      "outer fold 5/5 | tuning KNN      | inner ACC 90.42% | outer ACC 90.30%\n",
      "outer fold 5/5 | tuning Logistic | inner ACC 72.45% | outer ACC 71.80%\n",
      "outer fold 5/5 | tuning RandomForest | inner ACC 93.35% | outer ACC 92.60%\n",
      "CPU times: user 38.9 s, sys: 3.93 s, total: 42.8 s\n",
      "Wall time: 29min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# ^^ this handy Jupyter magic times the execution of the cell for you\n",
    "\n",
    "cv_scores = {name: [] for name, gs_est in gridcvs.items()}\n",
    "\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# The outer loop for algorithm selection\n",
    "c = 1\n",
    "for outer_train_idx, outer_valid_idx in skfold.split(X_train,y_train):\n",
    "    for name, gs_est in sorted(gridcvs.items()):\n",
    "        print('outer fold %d/5 | tuning %-8s' % (c, name), end='')\n",
    "\n",
    "        # The inner loop for hyperparameter tuning\n",
    "        gs_est.fit(X_train.iloc[outer_train_idx], y_train.iloc[outer_train_idx])\n",
    "        y_pred = gs_est.predict(X_train.iloc[outer_valid_idx])\n",
    "        acc = accuracy_score(y_true=y_train.iloc[outer_valid_idx], y_pred=y_pred)\n",
    "        print(' | inner ACC %.2f%% | outer ACC %.2f%%' %\n",
    "              (gs_est.best_score_ * 100, acc * 100))\n",
    "        cv_scores[name].append(acc)\n",
    "\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN      | outer CV acc. 91.18% +\\- 0.778\n",
      "RandomForest | outer CV acc. 93.96% +\\- 1.102\n",
      "Logistic | outer CV acc. 71.96% +\\- 1.555\n",
      "\n",
      "KNN best parameters {'classifier__n_neighbors': 20, 'classifier__weights': 'distance'}\n",
      "RandomForest best parameters {'classifier__max_features': 4}\n",
      "Logistic best parameters {'classifier__C': 100.0}\n"
     ]
    }
   ],
   "source": [
    "# Looking at the results\n",
    "for name in cv_scores:\n",
    "    print('%-8s | outer CV acc. %.2f%% +\\- %.3f' % (\n",
    "          name, 100 * np.mean(cv_scores[name]), 100 * np.std(cv_scores[name])))\n",
    "print()\n",
    "for name in cv_scores:\n",
    "    print('{} best parameters'.format(name), gridcvs[name].best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 91.80% (average over CV test folds)\n",
      "Best Parameters: {'classifier__n_neighbors': 20, 'classifier__weights': 'distance'}\n",
      "Training Accuracy: 100.00%\n",
      "Test Accuracy: 92.02%\n",
      "\n",
      "Accuracy 94.24% (average over CV test folds)\n",
      "Best Parameters: {'classifier__max_features': 4}\n",
      "Training Accuracy: 100.00%\n",
      "Test Accuracy: 94.66%\n",
      "\n",
      "Accuracy 72.14% (average over CV test folds)\n",
      "Best Parameters: {'classifier__C': 100.0}\n",
      "Training Accuracy: 72.40%\n",
      "Test Accuracy: 72.33%\n"
     ]
    }
   ],
   "source": [
    "# Fitting a model to the whole training set\n",
    "# using the \"best\" KNN algorithm\n",
    "best_algo = gridcvs['KNN']\n",
    "\n",
    "best_algo.fit(X_train, y_train)\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "print('Accuracy %.2f%% (average over CV test folds)' %\n",
    "      (100 * best_algo.best_score_))\n",
    "print('Best Parameters: %s' % gridcvs['KNN'].best_params_)\n",
    "print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "print('Test Accuracy: %.2f%%' % (100 * test_acc))\n",
    "knn_accuracy.append(test_acc)\n",
    "knn_accuracy_train.append(train_acc)\n",
    "\n",
    "# using the \"best\" RandomForest algorithm\n",
    "best_algo = gridcvs['RandomForest']\n",
    "\n",
    "best_algo.fit(X_train, y_train)\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "print('\\nAccuracy %.2f%% (average over CV test folds)' %\n",
    "      (100 * best_algo.best_score_))\n",
    "print('Best Parameters: %s' % gridcvs['RandomForest'].best_params_)\n",
    "print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "print('Test Accuracy: %.2f%%' % (100 * test_acc))\n",
    "randforest_accuracy.append(test_acc)\n",
    "randforest_accuracy_train.append(train_acc)\n",
    "\n",
    "# using the \"best\" Logistic algorithm\n",
    "best_algo = gridcvs['Logistic']\n",
    "\n",
    "best_algo.fit(X_train, y_train)\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "\n",
    "print('\\nAccuracy %.2f%% (average over CV test folds)' %\n",
    "      (100 * best_algo.best_score_))\n",
    "print('Best Parameters: %s' % gridcvs['Logistic'].best_params_)\n",
    "print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "print('Test Accuracy: %.2f%%' % (100 * test_acc))\n",
    "logreg_accuracy.append(test_acc)\n",
    "logreg_accuracy_train.append(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9202\n",
      "0.9466\n",
      "0.7233333333333334\n"
     ]
    }
   ],
   "source": [
    "print(knn_accuracy[2])\n",
    "print(randforest_accuracy[2])\n",
    "print(logreg_accuracy[2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average KNN Test Accuracy:  1.0\n",
      "Average Random Forest Test Accuracy:  1.0\n",
      "Average Logistic Regression Test Accuracy:  0.7258666666666667\n"
     ]
    }
   ],
   "source": [
    "#report average train accuracy per classifier (with best parameter)\n",
    "print(\"Average KNN Test Accuracy: \", np.mean(knn_accuracy_train))\n",
    "print(\"Average Random Forest Test Accuracy: \", np.mean(randforest_accuracy_train))\n",
    "print(\"Average Logistic Regression Test Accuracy: \", np.mean(logreg_accuracy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average KNN Test Accuracy:  0.9268\n",
      "Average Random Forest Test Accuracy:  0.9481111111111112\n",
      "Average Logistic Regression Test Accuracy:  0.7242888888888889\n"
     ]
    }
   ],
   "source": [
    "#report average test accuracy per classifier (with best parameter)\n",
    "print(\"Average KNN Test Accuracy: \", np.mean(knn_accuracy))\n",
    "print(\"Average Random Forest Test Accuracy: \", np.mean(randforest_accuracy))\n",
    "print(\"Average Logistic Regression Test Accuracy: \", np.mean(logreg_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T - test for LETTER dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-test between the different algorithms\n",
    "\n",
    "knn_forest = stats.ttest_ind(knn_accuracy, randforest_accuracy)\n",
    "\n",
    "forest_logistic = stats.ttest_ind(randforest_accuracy, logreg_accuracy)\n",
    "\n",
    "logistic_knn = stats.ttest_ind(logreg_accuracy, knn_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN and RandomForest:  Ttest_indResult(statistic=-6.018486029250711, pvalue=0.003838995866673888)\n",
      "\n",
      "RandomForest and Logistic Regression:  Ttest_indResult(statistic=201.03832462525068, pvalue=3.6725201869652128e-09)\n",
      "\n",
      "Logistic Regression and KNN:  Ttest_indResult(statistic=-57.1373229787318, pvalue=5.618046883959578e-07)\n"
     ]
    }
   ],
   "source": [
    "# Results of the T-tests\n",
    "\n",
    "print('KNN and RandomForest: ', knn_forest)\n",
    "print('\\nRandomForest and Logistic Regression: ', forest_logistic)\n",
    "print('\\nLogistic Regression and KNN: ', logistic_knn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
